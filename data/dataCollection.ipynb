{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79f47a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import PandasTools\n",
    "from rdkit.Chem import AllChem, MACCSkeys, rdMolDescriptors\n",
    "from rdkit.Chem.rdMolDescriptors import GetMorganFingerprintAsBitVect\n",
    "from rdkit.ML.Descriptors import MoleculeDescriptors\n",
    "from rdkit.Chem import Descriptors\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pubchempy as pcp\n",
    "import requests\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "base_dir = '/Users/samarjosyula/Desktop/PROJECTS/pgccInhibitorDrugDiscovery'\n",
    "data_path = os.path.join(base_dir, \"data\")\n",
    "if not os.path.exists(data_path): raise Exception(\"Path not found..\")\n",
    "result_path = os.path.join(base_dir, \"results\")\n",
    "if not os.path.exists(result_path): os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "descNames = [desc[0] for desc in Descriptors.descList]\n",
    "calc = MoleculeDescriptors.MolecularDescriptorCalculator(descNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e8d228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create compound library\n",
    "compoundData_Ma = pd.read_excel(\n",
    "    f'{data_path}/LiteratureData/Ma_compoundLibrary.xlsx',\n",
    "    sheet_name=1\n",
    ").rename(columns={'Item Name':'CompoundName', 'CAS Number':'casID'}).filter(items=['CompoundName', 'SMILES', 'Formula', 'casID'])\n",
    "\n",
    "compoundData_Zhou = pd.read_excel(\n",
    "    f'{data_path}/LiteratureData/Zhou1_compoundLibrary.xlsx',\n",
    "    header=1\n",
    ").rename(columns={'Compounds Name':'CompoundName', 'Item #':'ItemID'}).filter(items=['CompoundName', 'ItemID', 'Source', 'Target'])\n",
    "\n",
    "compoundData = pd.concat(\n",
    "    [\n",
    "        compoundData_Ma,\n",
    "        compoundData_Zhou\n",
    "    ], ignore_index=True, copy=True\n",
    ")\n",
    "compoundData.to_csv(f'{data_path}/compoundLibrary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af12a4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseDatabases(file=f'{data_path}/SupplementaryData/CaymanStructureDefinitions.sdf', verbose=False,):\n",
    "    suppl = Chem.SDMolSupplier(file)\n",
    "\n",
    "    # parse data from cayman database\n",
    "    caymanData = pd.DataFrame(columns=['CompoundName', 'SMILES'])\n",
    "    for mol in suppl:\n",
    "        if mol is not None and mol.HasProp('_Name'):\n",
    "            try:\n",
    "                name = mol.GetProp('Item name')\n",
    "                ItemID = mol.GetProp('Item number')\n",
    "                casID = mol.GetProp('CAS Number')\n",
    "                formula = mol.GetProp('Formula')\n",
    "                smi = Chem.MolToSmiles(mol)\n",
    "                caymanData = pd.concat([caymanData, pd.DataFrame([{'CompoundName':name, 'caymanID':ItemID, 'casID':casID, 'Formula':formula, 'SMILES':smi}])], ignore_index=True)\n",
    "            except:\n",
    "                continue\n",
    "    caymanData.drop_duplicates(subset='CompoundName', inplace=True)\n",
    "    if verbose: print('..parsed cayman database..')\n",
    "\n",
    "    # parse data from selleck database\n",
    "    selleck = pd.read_excel(\n",
    "        f'{data_path}/SupplementaryData/SelleckCompoundLibrary.xlsx', \n",
    "        sheet_name=1\n",
    "    )\n",
    "    selleckdf = selleck.rename(columns={'Cat':'selleckID', 'Name':'CompoundName', 'CAS Number':'casID'}).filter(items=['CompoundName', 'casID', 'selleckID', 'Formula', 'SMILES'])\n",
    "    if verbose: print('..parsed selleck database..\\n')\n",
    "\n",
    "    return caymanData, selleckdf\n",
    "\n",
    "# caymanData, selleckData = parseDatabases(verbose=True)\n",
    "# caymanData.to_csv(f'{data_path}/SupplementaryData/caymanData.csv')\n",
    "# selleckData.to_csv(f'{data_path}/SupplementaryData/selleckData.csv')\n",
    "\n",
    "caymanData = pd.read_csv(f'{data_path}/SupplementaryData/caymanData.csv')\n",
    "selleckData = pd.read_csv(f'{data_path}/SupplementaryData/selleckData.csv')\n",
    "\n",
    "# merge datasets\n",
    "allSuppData = caymanData.merge(selleckData, how='outer', on='SMILES')\n",
    "allSuppData['ItemID'] = allSuppData['caymanID'].fillna(allSuppData['selleckID'])\n",
    "allSuppData['CompoundName'] = allSuppData['CompoundName_x'].fillna(allSuppData['CompoundName_y'])\n",
    "allSuppData['Formula'] = allSuppData['Formula_x'].fillna(allSuppData['Formula_y'])\n",
    "mergedSuppData = allSuppData.filter(items=['CompoundName', 'SMILES', 'Formula', 'casID', 'ItemID'])\n",
    "\n",
    "mergedSuppData['CompoundName'] = mergedSuppData['CompoundName'].astype(str).str.strip()\n",
    "mergedSuppData['SMILES'] = mergedSuppData['SMILES'].astype(str).str.strip()\n",
    "mergedSuppData['ItemID'] = mergedSuppData['ItemID'].astype(str).str.strip()\n",
    "mergedSuppData.to_csv(f'{data_path}/SupplementaryData/mergedSuppData.csv')\n",
    "mergedSuppData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be235605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supplement data\n",
    "def supplementData(input, verbose=False):\n",
    "    compoundData = pd.DataFrame(input.copy())\n",
    "\n",
    "    # supplement smiles and molecular formula\n",
    "    for idx, row in compoundData.iterrows():\n",
    "        match = pd.DataFrame()\n",
    "        updated = False\n",
    "        if pd.notna(row.get('ItemID')):\n",
    "            match = caymanData[caymanData['caymanID'] == row['ItemID']] \n",
    "            if match.empty: # try match from selleck if couldn't match from cayman\n",
    "                match = selleckData[selleckData['selleckID'] == row['ItemID']] \n",
    "            if len(match) > 1: # more than one match found\n",
    "                pass \n",
    "            if match.empty: # no matches\n",
    "                match = pd.DataFrame()\n",
    "                updated=False\n",
    "            else: updated = True\n",
    "        elif (pd.isna(row.get('SMILES')) or pd.isna(row.get('Formula')) and pd.notna(row.get('casID'))):\n",
    "            match = caymanData[caymanData['casID'] == row['casID']]\n",
    "            updated = True\n",
    "\n",
    "        if updated and not match.empty:\n",
    "            nextRow = match.iloc[0]\n",
    "\n",
    "            # validate SMILES\n",
    "            mol = Chem.MolFromSmiles(nextRow['SMILES'])\n",
    "            if mol:\n",
    "                smi = Chem.MolToSmiles(mol, canonical=True, isomericSmiles=True)\n",
    "                smi = smi.encode('ascii', errors='ignore').decode()\n",
    "                nextRow['SMILES'] = smi\n",
    "            else:\n",
    "                nextRow['SMILES'] = None\n",
    "\n",
    "            compoundData.loc[\n",
    "                idx, ['Formula', 'SMILES']\n",
    "            ] = [nextRow['Formula'], nextRow['SMILES']]\n",
    "    if verbose: print('..supplemented SMILES and formula..')\n",
    "    compoundData = compoundData.drop(\n",
    "        columns=['casID', 'ItemID'], errors='ignore'\n",
    "    )\n",
    "    return compoundData\n",
    "\n",
    "compoundData = supplementData(compoundData, verbose=True)\n",
    "compoundData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd46e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "compoundLib = compoundData.drop_duplicates(subset='SMILES')\n",
    "print(compoundLib['SMILES'].isna().sum())\n",
    "compoundLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14914755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Inhibition Data\n",
    "def getInhibitionData(pubchemReqs=True, verbose=False):\n",
    "    inhibitorScreens = pd.read_excel(f'{data_path}/LiteratureData/Zhou3_PGCCinhibitorScreening.xlsx', header=1)\n",
    "    inhibitorScreens = inhibitorScreens.iloc[:, [9,10,11,12,13,14,15,16]].reset_index(drop=True,)\n",
    "    inhibitorScreens.columns = [\n",
    "        'Treatment',\n",
    "        'numPGCC : VARI068',\n",
    "        'numPGCC : SUM149',\n",
    "        'numPGCC : SUM159',\n",
    "        'numPGCC : MDA-MB231',\n",
    "        'Mean',\n",
    "        '95% Cl',\n",
    "        'p-value'\n",
    "    ]\n",
    "    inhibitorScreens = inhibitorScreens[inhibitorScreens['Treatment'] != 'Control'].reset_index(drop=True)\n",
    "    if verbose: print(f'..configured anti-PGCC screening data..(shape={inhibitorScreens.shape})\\n')\n",
    "\n",
    "    unmatched = []\n",
    "    if verbose: print('..configuring smiles..')\n",
    "    for idx, row in inhibitorScreens.iterrows():\n",
    "        name = str(row['Treatment']).strip()\n",
    "        \n",
    "        if verbose: print(f'Mol{idx+1}: {name}')\n",
    "\n",
    "        smi, target = None, None\n",
    "        filtered = mergedSuppData.loc[mergedSuppData['CompoundName'].str.lower()==name.lower()]\n",
    "\n",
    "        # troubleshooting\n",
    "        cleanedName = None\n",
    "        if filtered.empty: # remove parenthesis\n",
    "            noParens = re.sub(r'\\(.*?\\)', '', name).strip()\n",
    "            filtered = mergedSuppData.loc[mergedSuppData['CompoundName'].str.lower()==noParens.lower()]\n",
    "            cleanedName = noParens\n",
    "\n",
    "        if filtered.empty: # only inside parenthesis\n",
    "            onlyParens = re.findall(r'\\((.*?)\\)', name)\n",
    "            if onlyParens: \n",
    "                onlyParens = onlyParens[0].strip()\n",
    "                filtered = mergedSuppData.loc[mergedSuppData['CompoundName'].str.lower()==onlyParens.lower()]\n",
    "                cleanedName = onlyParens\n",
    "        \n",
    "        if filtered.empty: # only first word\n",
    "            firstWord = cleanedName.split(' ')[0]\n",
    "            filtered = mergedSuppData.loc[mergedSuppData['CompoundName'].str.lower()==firstWord.lower()]\n",
    "            cleanedName = firstWord\n",
    "\n",
    "        if filtered.empty: # substring match?? very loose..\n",
    "            filtered = mergedSuppData.loc[mergedSuppData['CompoundName'].str.lower().str.contains(cleanedName.lower())]\n",
    "            cleanedName = firstWord \n",
    "\n",
    "        smi = None\n",
    "        if not filtered.empty: \n",
    "            smiles = []\n",
    "            for fidx, frow in filtered.iterrows():\n",
    "                curr = str(frow['SMILES'])\n",
    "                try: \n",
    "                    mol = Chem.MolFromSmiles(curr)\n",
    "                    if mol: smiles.append(Chem.MolToSmiles(mol, canonical=True, isomericSmiles=True))\n",
    "                except: continue\n",
    "            if smiles: smi = max(smiles, key=len)\n",
    "        if smi is None or pd.isna(smi): unmatched.append(name)\n",
    "        \n",
    "        if verbose: print(f' > smi={smi}')\n",
    "        inhibitorScreens.at[idx, 'SMILES'] = smi\n",
    "\n",
    "    if verbose: print(f'\\n..acquired SMILES (n={len(inhibitorScreens[\"SMILES\"]) - inhibitorScreens[\"SMILES\"].isna().sum()})..')\n",
    "    if verbose: print(f'>unmatched: (n={len(unmatched)}):\\n{unmatched}')\n",
    "\n",
    "    # find SMILES for unmatched compounds\n",
    "    if pubchemReqs:\n",
    "        if verbose: print(f'\\n..finding unmatched smiles from pubchempy')\n",
    "        unmatchedCompounds = pd.DataFrame({'Treatment':unmatched}) \n",
    "        smilesList = []\n",
    "        unmatched.clear()\n",
    "        for idx, row in unmatchedCompounds.iterrows():\n",
    "            if verbose: print(f'Mol{idx+1}: {row[\"Treatment\"]}:')\n",
    "            name = re.sub(r'\\(.*?\\)', '', row['Treatment']).strip()\n",
    "            # name = row['Treatment']\n",
    "            try: \n",
    "                res = pcp.get_compounds(name, namespace='name', as_dataframe=True)\n",
    "                if not res.empty:\n",
    "                    smi = list(res['canonical_smiles'])[0]\n",
    "                    smilesList.append(smi)        \n",
    "                else:\n",
    "                    try: \n",
    "                        res = pcp.get_compounds(row['Treatment'], namespace='name', as_dataframe=True)\n",
    "                        if not res.empty:\n",
    "                            smi = list(res['canonical_smiles'])[0]\n",
    "                            smilesList.append(smi)        \n",
    "                        else:\n",
    "                            smi = None\n",
    "                            smilesList.append(None)\n",
    "                            unmatched.append(row['Treatment'])\n",
    "                    except Exception as e1: \n",
    "                        smi = None\n",
    "                        smilesList.append(None)\n",
    "                        unmatched.append(row['Treatment'])\n",
    "            except Exception as e:\n",
    "                smi = None\n",
    "                smilesList.append(None)\n",
    "                unmatched.append(row['Treatment'])\n",
    "            if verbose: print(f' > smi={smi}')\n",
    "        unmatchedCompounds['SMILES'] = smilesList\n",
    "\n",
    "        inhibitorScreens = inhibitorScreens.merge(\n",
    "            unmatchedCompounds[['Treatment', 'SMILES']], \n",
    "            how='left', \n",
    "            on='Treatment'\n",
    "        )\n",
    "\n",
    "        if 'SMILES_x' in inhibitorScreens.columns and 'SMILES_y' in inhibitorScreens.columns:\n",
    "            inhibitorScreens['SMILES'] = inhibitorScreens['SMILES_x'].combine_first(inhibitorScreens['SMILES_y'])\n",
    "            inhibitorScreens = inhibitorScreens.drop(columns=['SMILES_x', 'SMILES_y'])\n",
    "\n",
    "        if verbose: print(f'\\n..acquired {len(unmatchedCompounds)-unmatchedCompounds[\"SMILES\"].isna().sum()} from pubchempy..')\n",
    "        if verbose: print(f'>unmatched: (n={len(unmatched)}):\\n{unmatched}\\n')\n",
    "    elif verbose: print(f'Unmatched compounds (n={len(unmatched)}):\\n{unmatched}\\n')\n",
    "\n",
    "    # begin to gather data\n",
    "    aidsFromLib = compoundLib.copy().filter(items=['CompoundName', 'SMILES', 'CID', 'AID'])\n",
    "    inhibitorData = inhibitorScreens.filter(items=['Treatment', 'SMILES', 'p-value'])\n",
    "\n",
    "    # calculate labels and combine data\n",
    "    labels = pd.DataFrame(columns=['CompoundName', 'SMILES', 'p-value', 'anti-PGCC label'])\n",
    "    if verbose: print(f'..matching p-vals and smiles (n={len(inhibitorData)})..')\n",
    "    for idx, row in inhibitorData.iterrows():\n",
    "        name = row['Treatment']\n",
    "        smi = row['SMILES']\n",
    "\n",
    "        if verbose: print(f'Mol{idx+1}: {name}')\n",
    "        if pd.isna(smi) or smi is None: \n",
    "            if verbose: print(' - NaN SMILES (skipping)..')\n",
    "            continue\n",
    "        \n",
    "        cids = []\n",
    "        try:\n",
    "            cid = pcp.get_cids(\n",
    "                str(smi), \n",
    "                namespace='smiles', \n",
    "                domain='compound', \n",
    "                list_return='flat'\n",
    "            )\n",
    "            cids = cid\n",
    "        except Exception as e:\n",
    "            if verbose: print(f\" - CID lookup failed for {smi}: {e}\")\n",
    "\n",
    "        p = str(row['p-value'])\n",
    "        if str(p).startswith('<'): val = float(p[1:])\n",
    "        else: val = float(p)\n",
    "        \n",
    "        label = None\n",
    "        if 0 < val < 0.05: label = 1\n",
    "        elif val >= 0.05: label = 0\n",
    "        else: label, p = None, None\n",
    "\n",
    "        labels = pd.concat([\n",
    "            labels,\n",
    "            pd.DataFrame({'CompoundName':name, \n",
    "                          'SMILES':smi, \n",
    "                          'p-value':p, \n",
    "                          'anti-PGCC label':label\n",
    "                        }, index=[0])\n",
    "        ], ignore_index=True)\n",
    "\n",
    "    if verbose: print(f'\\n..configured PGCC inhibition labels (shape:{labels.shape})..')\n",
    "    return labels\n",
    "\n",
    "inhibitionLabels = getInhibitionData(verbose=True, pubchemReqs=True)\n",
    "inhibitionLabels.to_csv(f'{result_path}/inhibitionLabels.csv')\n",
    "inhibitionLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712d9eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inhibitionLabels = pd.read_csv(f'{result_path}/inhibitionLabels.csv')\n",
    "inhibitionLabels['anti-PGCC label'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89d54cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.DataStructs import ConvertToNumpyArray\n",
    "import numpy as np\n",
    "\n",
    "features_path = os.path.join(result_path, 'features')\n",
    "os.makedirs(features_path, exist_ok=True)\n",
    "\n",
    "smilabs = inhibitionLabels.copy()\n",
    "smilabs = smilabs.filter(items=['SMILES', 'anti-PGCC label']).rename(columns={'anti-PGCC label':'label'})\n",
    "\n",
    "rdkitRows = []\n",
    "maccsRows = []\n",
    "ecfp4Rows = []\n",
    "metaRows = []\n",
    "\n",
    "for idx, row in smilabs.iterrows():\n",
    "    smi = row['SMILES']\n",
    "    label = row['label']\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if not mol: continue\n",
    "\n",
    "    rdkit_desc = calc.CalcDescriptors(mol)\n",
    "\n",
    "    maccs = np.zeros((167,), dtype=int)\n",
    "    ConvertToNumpyArray(MACCSkeys.GenMACCSKeys(mol), maccs)\n",
    "\n",
    "    ecfp4 = np.zeros((2048,), dtype=int)\n",
    "    ConvertToNumpyArray(AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=2048), ecfp4)\n",
    "\n",
    "    rdkitRows.append([smi, label] + list(rdkit_desc))\n",
    "    maccsRows.append([smi, label] + list(maccs))\n",
    "    ecfp4Rows.append([smi, label] + list(ecfp4))\n",
    "    metaRows.append([smi, label] + list(maccs) + list(ecfp4))\n",
    "\n",
    "rdkitDF = pd.DataFrame(rdkitRows, columns= ['SMILES', 'label'] + descNames)\n",
    "maccsDF = pd.DataFrame(maccsRows, columns= ['SMILES', 'label'] + [f'MACCS_{i}' for i in range(167)])\n",
    "ecfp4DF = pd.DataFrame(ecfp4Rows, columns= ['SMILES', 'label'] + [f'ECFP4_{i}' for i in range(2048)])\n",
    "metaFeatures = pd.DataFrame(metaRows, columns= ['SMILES', 'label'] + [f'MACCS_{i}' for i in range(167)] + [f'ECFP4_{i}' for i in range(2048)])\n",
    "\n",
    "rdkitDF.to_csv(f'{features_path}/rdkit.csv')\n",
    "maccsDF.to_csv(f'{features_path}/maccs.csv')\n",
    "ecfp4DF.to_csv(f'{features_path}/ecfp4.csv')\n",
    "metaFeatures.to_csv(f'{features_path}/metaFigerprints.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfd9a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...create scaffold datasets for LLM4SD...\n",
    "\n",
    "# create paths to feed into LLM4SD\n",
    "LLM4SD_path = os.path.join(base_dir, 'LLM4SD_antiPGCC') # all data pertaining fed into LLM4SD\n",
    "os.makedirs(LLM4SD_path, exist_ok=True)\n",
    "scaffoldDatasets_path = os.path.join(LLM4SD_path, 'scaffold_datasets') # dataset directory\n",
    "os.makedirs(scaffoldDatasets_path, exist_ok=True)\n",
    "\n",
    "featuresDict = {\n",
    "    'rdkit' : rdkitDF, \n",
    "    'maccs' : maccsDF, \n",
    "    'ecfp4' : ecfp4DF, \n",
    "    'metaFingerprints' : metaFeatures\n",
    "}\n",
    "\n",
    "# create train/test/valid sets\n",
    "def splitSets(input: pd.DataFrame, name, path):\n",
    "    df = input.copy()\n",
    "\n",
    "    train, temp = train_test_split(df, test_size=0.2, random_state=15, stratify=df['label'], shuffle=True)\n",
    "    valid, test = train_test_split(temp, test_size=0.5, random_state=15, stratify=temp['label'], shuffle=True)\n",
    "\n",
    "    pd.DataFrame(train).to_csv(f'{path}/{name}_train.csv', index=False)\n",
    "    pd.DataFrame(test).to_csv(f'{path}/{name}_test.csv', index=False)\n",
    "    pd.DataFrame(valid).to_csv(f'{path}/{name}_valid.csv', index=False)\n",
    "\n",
    "for name, data in featuresDict.items():\n",
    "    df = pd.DataFrame(data, copy=True)\n",
    "    currPath = os.path.join(scaffoldDatasets_path, name)\n",
    "    os.makedirs(currPath, exist_ok=True)\n",
    "\n",
    "    if name=='rdkit': # split rdkit into further subcategories\n",
    "        smilab = df.filter(items=['SMILES', 'label'])\n",
    "\n",
    "        # E-State Descriptors\n",
    "        estate = [desc for desc in list(df.columns) if 'EState' in desc]\n",
    "        eStateDesc = pd.concat([smilab, df.filter(items=estate)], axis=1).reset_index(drop=True)\n",
    "        df = df.drop(columns=estate)\n",
    "        splitSets(input=eStateDesc, name='E-State', path=currPath)\n",
    "\n",
    "        # Functional Group Counts\n",
    "        funcGroup = [desc for desc in list(df.columns) if desc.startswith('fr_')]\n",
    "        frDesc = pd.concat([smilab, df.filter(items=funcGroup)], axis=1)\n",
    "        df = df.drop(columns=funcGroup)\n",
    "        splitSets(input=frDesc, name='functionalGroupCount', path=currPath)\n",
    "\n",
    "        # Molecular Topology Descriptors\n",
    "        topology = [desc for desc in list(df.columns) if desc.lower() in ['balabanj', 'bertzct', 'hallkieralpha', 'ipc', 'avgipc']]\n",
    "        topology += [desc for desc in list(df.columns) if desc.startswith('Chi') or desc.startswith('Kappa')]\n",
    "        topDesc = pd.concat([smilab, df.filter(items=topology)], axis=1)\n",
    "        df = df.drop(columns=topology)\n",
    "        splitSets(input=topDesc, name='molecularTopology', path=currPath)\n",
    "\n",
    "        # Fingerprint Based Descriptors\n",
    "        fing = [desc for desc in list(df.columns) if desc.startswith('Fp') or desc.startswith('BCUT2D')]\n",
    "        fingDesc = pd.concat([smilab, df.filter(items=fing)], axis=1)\n",
    "        df = df.drop(columns=fing)\n",
    "        splitSets(input=fingDesc, name='fingerprintBased', path=currPath)\n",
    "\n",
    "        # Surface Area Descriptors\n",
    "        sa = [desc for desc in list(df.columns) if any(x in desc.lower() for x in ['peoe', 'smr', 'slogp']) or desc.lower() == 'labuteasa']\n",
    "        saDesc = pd.concat([smilab, df.filter(items=sa)], axis=1)\n",
    "        df = df.drop(columns=sa)\n",
    "        splitSets(input=saDesc, name='surfaceArea', path=currPath)\n",
    "\n",
    "        # structural descriptors and counts\n",
    "        sdc = [\n",
    "            desc for desc in list(df.columns)\n",
    "            if (\n",
    "                desc.lower().startswith('n') \n",
    "                and desc.lower() not in ['numvalenceelectrons', 'numradicalelectrons']\n",
    "            ) or desc.lower() in ['fractioncsp3', 'ringcount']\n",
    "        ]\n",
    "        sdcDesc = pd.concat([smilab, df.filter(items=sdc)], axis=1)\n",
    "        df = df.drop(columns=sdc)\n",
    "        splitSets(input=sdcDesc, name='structural', path=currPath)\n",
    "\n",
    "        # physiochemical descriptors\n",
    "        splitSets(input=df, name='physiochemical', path=currPath)\n",
    "\n",
    "    else: splitSets(input=df, name=name, path=currPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137987a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create descriptors and split into train/test/valid sets\n",
    "# split up into maccs, ecfp4, rdkit, and combined fingerprints\n",
    "# place into LLM4Sd directory to parse into src/tools\n",
    "\n",
    "# RUN ON GOOGLE COLLAB\n",
    "# YOU DO NOT HAVE THE RAM FOR THIS PROJECT\n",
    "\n",
    "# create run.ipynb, and maybe individual run files, that iterate through the classes in src/tools"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "antiPGCCvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
